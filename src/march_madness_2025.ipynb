{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# March Madness 2025 Prediction Pipeline\n",
    "\n",
    "This notebook orchestrates the complete end-to-end prediction process for the 2025 March Madness tournaments (both men's and women's). It uses the ELO-enhanced machine learning approach to generate predictions.\n",
    "\n",
    "## Pipeline Steps\n",
    "1. Load and preprocess data for both men's and women's tournaments\n",
    "2. Calculate ELO ratings and advanced statistics\n",
    "3. Train ELO-enhanced ML models for both tournaments\n",
    "4. Generate predictions for all possible matchups\n",
    "5. Identify and visualize predictions for actual tournament teams\n",
    "6. Create consolidated submission file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries and Configure Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable auto-reloading of modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Standard libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set a nice plot style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Import our custom classes\n",
    "from data_classes.processing import (\n",
    "    MarchMadnessDataManager, \n",
    "    EloRatingSystem, \n",
    "    TeamStatsCalculator,\n",
    "    MarchMadnessMLModel,\n",
    "    MarchMadnessPredictor\n",
    ")\n",
    "from data_classes.bracket import BracketSimulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CURRENT_SEASON = 2025                 # Target season\n",
    "DATA_DIR = f\"../data/{CURRENT_SEASON}\"    # Directory for men's tournament data\n",
    "ELO_START_YEAR = 2003                 # First year for ELO calculations\n",
    "ML_START_YEAR = 2019                 # First year for ML training\n",
    "SUBMISSION_FILE = \"output/submission_2025.csv\"  # Combined submission file\n",
    "PERFORM_TUNING = False\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(\"output\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Tournament Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data manager for men's tournament\n",
    "men_data_manager = MarchMadnessDataManager(DATA_DIR, \"M\", CURRENT_SEASON)\n",
    "\n",
    "# Load all data files for men's tournament\n",
    "men_data = men_data_manager.load_data()\n",
    "\n",
    "# Display summary of loaded data\n",
    "print(\"Loaded men's tournament data files:\")\n",
    "for key, df in men_data.items():\n",
    "    print(f\"  - {key}: {df.shape[0]} rows, {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ELO rating systems (with temporary default parameters)\n",
    "men_elo_system = EloRatingSystem(men_data_manager)\n",
    "\n",
    "# Initialize stats calculators (don't calculate stats yet)\n",
    "men_stats_calculator = TeamStatsCalculator(men_data_manager)\n",
    "\n",
    "# Initialize ML models (without training them)\n",
    "men_ml_model = MarchMadnessMLModel(men_data_manager, men_elo_system, men_stats_calculator)\n",
    "\n",
    "# Initialize predictors\n",
    "men_predictor = MarchMadnessPredictor(\n",
    "    data_manager=men_data_manager,\n",
    "    elo_system=men_elo_system,\n",
    "    stats_calculator=men_stats_calculator,\n",
    "    ml_model=men_ml_model,\n",
    "    current_season=CURRENT_SEASON\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data manager for women's tournament\n",
    "women_data_manager = MarchMadnessDataManager(DATA_DIR, \"W\", CURRENT_SEASON)\n",
    "\n",
    "# Load all data files for women's tournament\n",
    "women_data = women_data_manager.load_data()\n",
    "\n",
    "# Display summary of loaded data\n",
    "print(\"Loaded women's tournament data files:\")\n",
    "for key, df in women_data.items():\n",
    "    print(f\"  - {key}: {df.shape[0]} rows, {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "women_stats_calculator = TeamStatsCalculator(women_data_manager)\n",
    "women_elo_system = EloRatingSystem(women_data_manager)\n",
    "women_ml_model = MarchMadnessMLModel(women_data_manager, women_elo_system, women_stats_calculator)\n",
    "women_predictor = MarchMadnessPredictor(\n",
    "    data_manager=women_data_manager,\n",
    "    elo_system=women_elo_system,\n",
    "    stats_calculator=women_stats_calculator,\n",
    "    ml_model=women_ml_model,\n",
    "    current_season=CURRENT_SEASON\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PERFORM_TUNING:\n",
    "    TEST_SEASONS = [2015, 2016, 2017, 2018, 2019, 2021, 2022, 2023, 2024]\n",
    "    # Perform ELO parameter tuning\n",
    "    print(\"Tuning ELO parameters for men's tournament...\")\n",
    "    # For faster testing, use a smaller parameter grid and fewer test seasons\n",
    "    men_elo_tuning = men_predictor.tune_elo_parameters(\n",
    "        test_seasons=TEST_SEASONS,  # Adjust years as needed\n",
    "    )\n",
    "\n",
    "    # Get optimal parameters\n",
    "    men_best_k = men_elo_tuning.iloc[0]['k_factor']\n",
    "    men_best_rf = men_elo_tuning.iloc[0]['recency_factor']\n",
    "    men_best_rw = men_elo_tuning.iloc[0]['recency_window']\n",
    "    men_best_co = men_elo_tuning.iloc[0]['carry_over_factor']\n",
    "    print(f\"Optimal men's parameters: k={men_best_k}, rf={men_best_rf}, rw={men_best_rw}, co={men_best_co}\")\n",
    "\n",
    "    # Same for women\n",
    "    print(\"\\nTuning ELO parameters for women's tournament...\")\n",
    "    women_elo_tuning = women_predictor.tune_elo_parameters(\n",
    "        test_seasons=TEST_SEASONS # Adjust years as needed\n",
    "    )\n",
    "\n",
    "    # Get optimal parameters\n",
    "    women_best_k = women_elo_tuning.iloc[0]['k_factor']\n",
    "    women_best_rf = women_elo_tuning.iloc[0]['recency_factor'] \n",
    "    women_best_rw = women_elo_tuning.iloc[0]['recency_window']\n",
    "    women_best_co = women_elo_tuning.iloc[0]['carry_over_factor']\n",
    "    print(f\"Optimal women's parameters: k={women_best_k}, rf={women_best_rf}, rw={women_best_rw}, co={women_best_co}\")\n",
    "\n",
    "    # Save tuning results\n",
    "    men_elo_tuning.to_csv('output/men_elo_tuning_results.csv', index=False)\n",
    "    women_elo_tuning.to_csv('output/women_elo_tuning_results.csv', index=False)\n",
    "else:\n",
    "    print(\"Skipping parameter tuning. Using pre-defined values.\")\n",
    "    # Use fixed parameter values (e.g. from previous tuning runs)\n",
    "    men_best_k = 10 \n",
    "    men_best_rf = 3.0\n",
    "    men_best_rw = 25\n",
    "    men_best_co = 0.7\n",
    "    women_best_k = 40\n",
    "    women_best_rf = 3.0\n",
    "    women_best_rw = 20\n",
    "    women_best_co = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Men's Tournament"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "men_elo_ratings = men_elo_system.calculate_elo_ratings(\n",
    "    start_year=ELO_START_YEAR,\n",
    "    k_factor=men_best_k,            # Use optimized value\n",
    "    recency_factor=men_best_rf,     # Use optimized value\n",
    "    recency_window=men_best_rw,     # Use optimized value\n",
    "    home_advantage=100,\n",
    "    carry_over_factor=men_best_co,\n",
    "    output_path=\"output/M_elo_ratings.csv\"\n",
    ")\n",
    "\n",
    "# Get current ELO ratings for the tournament teams\n",
    "men_current_elo = men_elo_system.get_all_teams_elo(CURRENT_SEASON)\n",
    "\n",
    "# Display top 10 men's teams by ELO rating\n",
    "print(\"\\nTop 10 men's teams by ELO rating:\")\n",
    "display(men_current_elo.sort_values('ELO', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize stats calculator for men's tournament\n",
    "men_stats_calculator = TeamStatsCalculator(men_data_manager)\n",
    "\n",
    "try:\n",
    "    # Calculate advanced statistics\n",
    "    print(\"Calculating advanced statistics for men's teams...\")\n",
    "    men_advanced_stats = men_stats_calculator.calculate_advanced_team_stats(start_season=ELO_START_YEAR)\n",
    "    \n",
    "    # Success message\n",
    "    print(f\"Calculated advanced statistics for {len(men_advanced_stats)} seasons\")\n",
    "except Exception as e:\n",
    "    print(f\"Error calculating advanced stats: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ML model for men's tournament\n",
    "men_ml_model = MarchMadnessMLModel(\n",
    "    men_data_manager,\n",
    "    men_elo_system,\n",
    "    men_stats_calculator\n",
    ")\n",
    "# Train ELO-enhanced ML model\n",
    "print(\"Training ELO-enhanced ML model for men's tournament...\")\n",
    "men_ml_model.train_model(\n",
    "    model_type='xgboost',\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the predictor for men's tournament\n",
    "men_predictor = MarchMadnessPredictor(\n",
    "    data_manager=men_data_manager,\n",
    "    elo_system=men_elo_system,\n",
    "    stats_calculator=men_stats_calculator,\n",
    "    ml_model=men_ml_model,\n",
    "    current_season=CURRENT_SEASON\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Women's Tournament Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "women_elo_ratings = women_elo_system.calculate_elo_ratings(\n",
    "    start_year=ELO_START_YEAR,\n",
    "    k_factor=women_best_k,            # Use optimized value\n",
    "    recency_factor=women_best_rf,     # Use optimized value\n",
    "    recency_window=women_best_rw,     # Use optimized value\n",
    "    home_advantage=100,\n",
    "    carry_over_factor=women_best_co,\n",
    "    output_path=\"output/W_elo_ratings.csv\"\n",
    ")\n",
    "\n",
    "# Get current ELO ratings for the tournament teams\n",
    "women_current_elo = women_elo_system.get_all_teams_elo(CURRENT_SEASON)\n",
    "\n",
    "# Display top 10 women's teams by ELO rating\n",
    "print(\"\\nTop 10 women's teams by ELO rating:\")\n",
    "display(women_current_elo.sort_values('ELO', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize stats calculator for women's tournament\n",
    "women_stats_calculator = TeamStatsCalculator(women_data_manager)\n",
    "\n",
    "try:\n",
    "    # Calculate advanced statistics\n",
    "    print(\"Calculating advanced statistics for women's teams...\")\n",
    "    women_advanced_stats = women_stats_calculator.calculate_advanced_team_stats(start_season=ELO_START_YEAR)\n",
    "    \n",
    "    # Success message\n",
    "    print(f\"Calculated advanced statistics for {len(women_advanced_stats)} seasons\")\n",
    "except Exception as e:\n",
    "    print(f\"Error calculating advanced stats: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ML model for women's tournament\n",
    "women_ml_model = MarchMadnessMLModel(\n",
    "    women_data_manager,\n",
    "    women_elo_system,\n",
    "    women_stats_calculator\n",
    ")\n",
    "\n",
    "# Train ELO-enhanced ML model\n",
    "print(\"Training ELO-enhanced ML model for women's tournament...\")\n",
    "women_ml_model.train_model(\n",
    "    model_type='xgboost',\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the predictor for women's tournament\n",
    "women_predictor = MarchMadnessPredictor(\n",
    "    data_manager=women_data_manager,\n",
    "    elo_system=women_elo_system,\n",
    "    stats_calculator=women_stats_calculator,\n",
    "    ml_model=women_ml_model,\n",
    "    current_season=CURRENT_SEASON\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate Model Performance with Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_historical_tournaments(predictor, seasons=[2018, 2019, 2021, 2022, 2023], method=\"elo_enhanced\", gender=\"M\"):\n",
    "    \"\"\"Evaluate and visualize predictions for multiple historical tournaments\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "    from IPython.display import display\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for season in seasons:\n",
    "        try:\n",
    "            print(f\"\\nEvaluating {season} tournament predictions...\")\n",
    "            \n",
    "            # Create simulator for this season\n",
    "            simulator = BracketSimulator(predictor)\n",
    "            simulator.use_predictor_data(season=season)\n",
    "            \n",
    "            # Visualize historical bracket\n",
    "            fig = simulator.visualize_historical_bracket(\n",
    "                season,\n",
    "                method=method,\n",
    "                output_path=f\"output/{gender}_historical_{season}_bracket.png\"\n",
    "            )\n",
    "            \n",
    "            # Get accuracy metrics from simulation\n",
    "            _, metrics = simulator.simulate_historical_bracket(season, method=method)\n",
    "            results.append({\"season\": season, **metrics})\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {season} tournament: {str(e)}\")\n",
    "    \n",
    "    # Display aggregate results\n",
    "    if results:\n",
    "        # Create a DataFrame\n",
    "        results_df = pd.DataFrame(results)\n",
    "        \n",
    "        # Calculate aggregate metrics\n",
    "        total_correct = results_df['correct'].sum()\n",
    "        total_games = results_df['total'].sum()\n",
    "        overall_accuracy = total_correct / total_games if total_games > 0 else 0\n",
    "        \n",
    "        print(\"\\nHistorical Tournament Prediction Results:\")\n",
    "        print(f\"Overall Accuracy: {overall_accuracy:.2%} ({total_correct}/{total_games} games)\")\n",
    "        print(\"\\nBy Season:\")\n",
    "        display(results_df)\n",
    "        \n",
    "        # Plot accuracy by season\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(results_df['season'], results_df['accuracy'])\n",
    "        plt.xlabel('Tournament Season')\n",
    "        plt.ylabel('Prediction Accuracy')\n",
    "        plt.title(f'Tournament Prediction Accuracy by Season ({method})')\n",
    "        plt.ylim(0, 1)\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Add accuracy labels on top of the bars\n",
    "        for i, row in results_df.iterrows():\n",
    "            plt.text(row['season'], row['accuracy'] + 0.02, \n",
    "                     f\"{row['accuracy']:.2%}\\n({row['correct']}/{row['total']})\",\n",
    "                     ha='center')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "men_historical_results = evaluate_historical_tournaments(\n",
    "    men_predictor,  # Your existing predictor instance\n",
    "    seasons=[2021, 2022, 2023, 2024],  # Skip 2020 (cancelled)\n",
    "    method=\"elo_enhanced\"  # Use your best method\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "women_historical_results = evaluate_historical_tournaments(\n",
    "    women_predictor,  # Your existing predictor instance\n",
    "    seasons=[2021, 2022, 2023, 2024],  # Skip 2020 (cancelled)\n",
    "    method=\"elo_enhanced\",  # Use your best method\n",
    "    gender=\"W\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare men's prediction methods on historical tournaments\n",
    "print(\"Comparing prediction methods for men's tournament...\")\n",
    "men_comparison = men_predictor.compare_methods(visualize=True)\n",
    "\n",
    "# Extract the best method for men's predictions\n",
    "men_best_method = men_comparison.loc[men_comparison['Log Loss'].idxmin(), 'Method']\n",
    "print(f\"\\nBest method for men's tournament based on backtesting: {men_best_method}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare women's prediction methods on historical tournaments\n",
    "print(\"Comparing prediction methods for women's tournament...\")\n",
    "women_comparison = women_predictor.compare_methods(visualize=True)\n",
    "\n",
    "# Extract the best method for women's predictions\n",
    "women_best_method = women_comparison.loc[women_comparison['Log Loss'].idxmin(), 'Method']\n",
    "print(f\"\\nBest method for women's tournament based on backtesting: {women_best_method}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Predictions for Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate men's tournament predictions using the best method\n",
    "print(f\"Generating men's tournament predictions using {men_best_method} method...\")\n",
    "men_predictions = men_predictor.generate_predictions(\n",
    "    method=men_best_method,\n",
    "    get_all_matchups=True,\n",
    "    submission_file=\"output/submission_2025_M.csv\"\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(men_predictions)} predictions for men's tournament\")\n",
    "display(men_predictions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate women's tournament predictions using the best method\n",
    "print(f\"Generating women's tournament predictions using {women_best_method} method...\")\n",
    "women_predictions = women_predictor.generate_predictions(\n",
    "    method=women_best_method,\n",
    "    get_all_matchups=True,\n",
    "    submission_file=\"output/submission_2025_W.csv\"\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(women_predictions)} predictions for women's tournament\")\n",
    "display(women_predictions.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Combined Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract required columns for submission\n",
    "men_submission = men_predictions[['ID', 'Pred']].copy()\n",
    "women_submission = women_predictions[['ID', 'Pred']].copy()\n",
    "\n",
    "# Combine men's and women's predictions\n",
    "combined_submission = pd.concat([men_submission, women_submission])\n",
    "\n",
    "# Drop any duplicates (should not happen, but just in case)\n",
    "combined_submission = combined_submission.drop_duplicates(subset=['ID'])\n",
    "\n",
    "# Save combined submission file\n",
    "combined_submission.to_csv(SUBMISSION_FILE, index=False)\n",
    "\n",
    "print(f\"Saved {len(combined_submission)} predictions to {SUBMISSION_FILE}\")\n",
    "print(f\"  - Men's predictions: {len(men_submission)}\")\n",
    "print(f\"  - Women's predictions: {len(women_submission)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Tournament Team Prediction and Visualization\n",
    "\n",
    "This section will work once the tournament bracket is released with the actual tournament teams."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Visualize Men's Tournament Bracket\ntry:\n    # Generate predictions for actual tournament teams\n    # men_tourney_predictions = men_predictor.predict_tournament_bracket(method=men_best_method)\n    # print(f\"Generated predictions for {len(men_tourney_predictions)//2} matchups between men's tournament teams\")\n    \n    # Initialize the bracket simulator\n    men_simulator = BracketSimulator(men_predictor)\n    men_simulator.use_predictor_data()\n    \n    # Build the bracket structure\n    men_bracket_tree, men_seed_slot_map = men_simulator.build_bracket_tree()\n    \n    # Generate the bracket visualization\n    print(\"Generating men's tournament bracket visualization...\")\n    # Use submission file for accurate probabilities\n    submission_file = f\"output/submission_{CURRENT_SEASON}_M.csv\"\n    if not os.path.exists(submission_file):\n        submission_file = f\"output/submission_{CURRENT_SEASON}.csv\"\n        \n    men_simulator.visualize_bracket(\n        method=men_best_method,\n        output_path=f\"output/{CURRENT_SEASON}_mens_bracket.png\",\n        betting_odds=True,\n        submission_file=submission_file\n    )\n\n    men_simulator.visualize_bracket(\n        method=men_best_method,\n        output_path=f\"output/{CURRENT_SEASON}_mens_bracket_prob.png\",\n        betting_odds=False,\n        submission_file=submission_file\n    )\n    \n    print(f\"Saved men's bracket visualization to output/{CURRENT_SEASON}_mens_bracket.png\")\nexcept Exception as e:\n    print(f\"Could not generate men's tournament bracket: {e}\")\n    print(\"This functionality will work once the tournament bracket is released.\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Visualize Women's Tournament Bracket\ntry:\n    # Generate predictions for actual tournament teams\n    women_tourney_predictions = women_predictor.predict_tournament_bracket(method=women_best_method)\n    print(f\"Generated predictions for {len(women_tourney_predictions)//2} matchups between women's tournament teams\")\n    \n    # Initialize the bracket simulator\n    women_simulator = BracketSimulator(women_predictor)\n    women_simulator.use_predictor_data()\n    \n    # Build the bracket structure\n    women_bracket_tree, women_seed_slot_map = women_simulator.build_bracket_tree()\n    \n    # Generate the bracket visualization\n    print(\"Generating women's tournament bracket visualization...\")\n    # Use submission file for accurate probabilities\n    submission_file = f\"output/submission_{CURRENT_SEASON}_W.csv\"\n    if not os.path.exists(submission_file):\n        submission_file = f\"output/submission_{CURRENT_SEASON}.csv\"\n        \n    women_simulator.visualize_bracket(\n        method=women_best_method,\n        output_path=f\"output/{CURRENT_SEASON}_womens_bracket.png\",\n        betting_odds=True,\n        submission_file=submission_file\n    )\n    \n    print(f\"Saved women's bracket visualization to output/{CURRENT_SEASON}_womens_bracket.png\")\nexcept Exception as e:\n    print(f\"Could not generate women's tournament bracket: {e}\")\n    print(\"This functionality will work once the tournament bracket is released.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to explore a specific team's path through the tournament\n",
    "def explore_team_path(team_name, simulator: BracketSimulator, data_manager: MarchMadnessDataManager, gender=\"Men's\"):\n",
    "    # Get the team ID from the name\n",
    "    team_id = None\n",
    "    teams_df = data_manager.data['teams']\n",
    "    matching_teams = teams_df[teams_df['TeamName'].str.contains(team_name, case=False)]\n",
    "    \n",
    "    if len(matching_teams) == 0:\n",
    "        print(f\"No {gender.lower()} teams found matching '{team_name}'\")\n",
    "        return\n",
    "    elif len(matching_teams) > 1:\n",
    "        print(f\"Multiple {gender.lower()} teams found matching '{team_name}':\")\n",
    "        for _, team in matching_teams.iterrows():\n",
    "            print(f\"  - {team['TeamName']} (ID: {team['TeamID']})\")\n",
    "        team_id = matching_teams.iloc[0]['TeamID']\n",
    "        print(f\"Using first match: {matching_teams.iloc[0]['TeamName']} (ID: {team_id})\")\n",
    "    else:\n",
    "        team_id = matching_teams.iloc[0]['TeamID']\n",
    "    \n",
    "    try:\n",
    "        # Get the team's seed\n",
    "        team_seed = data_manager.seed_lookup.get((CURRENT_SEASON, team_id), \"Unknown\")\n",
    "        print(f\"\\nExploring {gender} tournament path for {team_seed} seed {data_manager.get_team_name(team_id)} (ID: {team_id})\")\n",
    "        \n",
    "        # Get the team's tournament path\n",
    "        path = simulator.get_team_path(team_id)\n",
    "        \n",
    "        # Create a DataFrame for the path\n",
    "        path_df = pd.DataFrame([\n",
    "            {\n",
    "                \"Round\": p[\"round\"],\n",
    "                \"Opponent\": p[\"opponent_name\"],\n",
    "                \"Opponent Seed\": p[\"opponent_seed\"],\n",
    "                \"Win Probability\": f\"{p['win_probability']:.1%}\"\n",
    "            }\n",
    "            for p in path\n",
    "        ])\n",
    "        \n",
    "        display(path_df)\n",
    "        \n",
    "        # Calculate championship probability\n",
    "        champ_prob = np.prod([p[\"win_probability\"] for p in path])\n",
    "        print(f\"Championship probability: {champ_prob:.2%}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting team path: {e}\")\n",
    "        print(\"This functionality will work once the tournament bracket is released.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enhanced_bracket_analysis import generate_enhanced_analysis\n",
    "# Pass your already-initialized predictor with all data loaded\n",
    "output_file = generate_enhanced_analysis(\n",
    "    ml_model=men_ml_model,\n",
    "    simulator=men_simulator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "### Model Approach\n",
    "We've implemented an ELO-enhanced ML approach that uses ELO ratings as the foundation and machine learning to predict corrections to those ratings. This hybrid approach leverages the strengths of both methods:\n",
    "\n",
    "- ELO provides reliable baseline predictions based on historical performance\n",
    "- ML model learns patterns that ELO misses (like matchup-specific factors)\n",
    "- System falls back to ELO when ML is uncertain or lacks data\n",
    "\n",
    "### Files Generated\n",
    "\n",
    "- `output/submission_2025.csv`: Combined men's and women's predictions for all possible matchups\n",
    "- `output/submission_2025_M.csv`: Men's tournament specific predictions\n",
    "- `output/submission_2025_W.csv`: Women's tournament specific predictions\n",
    "- `output/2025_mens_bracket.png`: Visualized men's tournament bracket with predictions\n",
    "- `output/2025_womens_bracket.png`: Visualized women's tournament bracket with predictions\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Once tournament brackets are released, run the visualization cells again to see predictions for actual tournament teams\n",
    "2. Use the explore_team_path function to analyze specific teams of interest\n",
    "3. Submit the combined predictions file to the competition\n",
    "\n",
    "### Performance Metrics\n",
    "\n",
    "The backtesting results indicate that the ELO-enhanced approach generally outperforms pure ELO, particularly in terms of log loss which is the competition's scoring metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}